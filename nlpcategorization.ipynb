{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875c534c",
   "metadata": {},
   "source": [
    "# Data parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2e79a",
   "metadata": {},
   "source": [
    "For extracting all the useful and significant text from the web page, we need to find all the tags which are created to store it. \n",
    "Such as: \n",
    "1. All levels ```<h>``` for headlines\n",
    "2. ```<p>``` for paragraph of text\n",
    "Another tags for text are wrappers for ```<p>```, so we will take the information out by using  ```<p>``` tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0b80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86f252ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datascraping (url):\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        soup = bs(r.text, 'html.parser')\n",
    "        #add all tags about text\n",
    "        allp = soup.findAll('p')\n",
    "        sitetext = ''\n",
    "        for i in range(1,7):\n",
    "            allh = soup.findAll(f'h{i}')\n",
    "            for h in allh:\n",
    "                sitetext += h.text\n",
    "        for p in allp:\n",
    "            sitetext += p.text \n",
    "        return sitetext\n",
    "    else:\n",
    "        print ('No server connection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb26b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitetext = datascraping('https://oxylabs.io/blog/python-web-scraping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a46ee5",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b594774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8556f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocessing(sitetext):\n",
    "    sitetext = re.sub(r'[^\\w\\s]','', sitetext)\n",
    "    sitetext = nlp(sitetext)\n",
    "    tokens = []\n",
    "    tokens.append([token.lemma_ for token in sitetext if token.is_stop == False])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dcaf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitetext = datapreprocessing(sitetext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fe58d",
   "metadata": {},
   "source": [
    "# Topics modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "070faddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "dictionary  = corpora.Dictionary(sitetext)\n",
    "corpus = [dictionary.doc2bow(text) for text in sitetext]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f49ff7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72ba4f61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_document_topics() missing 1 required positional argument: 'bow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ty/zhdd1dkj36j5r796hbm4035c0000gn/T/ipykernel_96048/2409424789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_document_topics() missing 1 required positional argument: 'bow'"
     ]
    }
   ],
   "source": [
    "lda_model.get_document_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca3d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
